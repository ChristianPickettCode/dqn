{
  "cells": [
    {
      "cell_type": "code",
      "source": "!pip3 install gym==0.26.2",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698259272008,
        "execution_millis": 1650,
        "is_output_hidden": true,
        "deepnote_to_be_reexecuted": false,
        "deepnote_app_is_output_hidden": true,
        "cell_id": "502bb001118c464693a3460fba401d0a",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "028031a57b2948f8ac0976f7511572a5"
    },
    {
      "cell_type": "code",
      "source": "import gym\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nfrom collections import deque\nfrom mpl_toolkits import mplot3d\nfrom matplotlib import cm\nimport pandas as pd\nimport seaborn as sns",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698259273665,
        "execution_millis": 3146,
        "is_output_hidden": true,
        "deepnote_to_be_reexecuted": false,
        "deepnote_app_is_output_hidden": true,
        "cell_id": "056714d2b99f465c9dba6d11c5235f5e",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "fe76a0a7b7524979a6b15b7b71b3ef21"
    },
    {
      "cell_type": "code",
      "source": "# Set the seed for reproducibility\nseed = 7\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nnp.random.seed(seed)\nrandom.seed(seed)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698259276818,
        "execution_millis": 31,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "12dade3abfc0488084ef3bde421a76db",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "6870a16a8107490f974b91b7969544d9"
    },
    {
      "cell_type": "markdown",
      "source": "### Simple Deep RL Algorithm",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "aaaf5b6e26a64ab5b003d56b1c170c06",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "f3ba277d02b840908a36fcde9460029a"
    },
    {
      "cell_type": "markdown",
      "source": "- Episodic - fix number of time steps to act in environment, not continuous",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b2e7d6ae6567491da32ed6a0cf97c2c6",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "71e33a43ac4f4e64ad12f8a2c865227e"
    },
    {
      "cell_type": "markdown",
      "source": "- Online - Agent as access to the environment and acts (interacts) on the env in real time",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "eaa8ac432a9b434c8e3254590aa884d3",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "acb54309e05c4163985b1506e152807e"
    },
    {
      "cell_type": "markdown",
      "source": "- Mode-free - We don't try to create an internal model of the environment",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "5a181dce18ad476ea05d82fbe0580862",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "e25764cf6b4d48d886fd41813879c38b"
    },
    {
      "cell_type": "code",
      "source": "class DQN(nn.Module):\n    def __init__(self, env, learning_rate):\n        super(DQN, self).__init__()\n\n        # Input to DNN - input features (observations)\n        input_features = env.observation_space.shape[0]\n\n        # Output to DNN - output actions\n        action_space = env.action_space.n\n\n        # i - 128 - 64 - 32 - o\n        self.dense1 = nn.Linear(in_features=input_features, out_features=128)\n        self.dense2 = nn.Linear(in_features=128, out_features=64)\n        self.dense3 = nn.Linear(in_features=64, out_features=32)\n        self.dense4 = nn.Linear(in_features=32, out_features=action_space)\n\n        # How does the Adam optimizer work?\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n    \n    def forward(self, x):\n        # Why tanh? try with relu instead of tanh too\n        output = torch.relu(self.dense1(x))\n        output = torch.tanh(self.dense2(output))\n        output = torch.tanh(self.dense3(output))\n        output = self.dense4(output)\n        \n        return output\n",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698259276830,
        "execution_millis": 28,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "dc39fa7fa9134423b7d68c1f6a422f0e",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "8c177818802c4bec8956cfb5df5baa58"
    },
    {
      "cell_type": "markdown",
      "source": "### Experience Replay",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "cell_id": "d910552cab524119af9ac3bc5d0ae97c",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "170f3bb491b1433f84e55f923934af1e"
    },
    {
      "cell_type": "markdown",
      "source": "- Data assumption for gradient method for training DNN : iid (independent identically distributed) - Not true for reinforcement learning (why?)",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "cell_id": "9698126bd9934b138df7cc84913ef080",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "108446af3e954963a56431b6b3f14d60"
    },
    {
      "cell_type": "markdown",
      "source": "- Because the data is highly correlated the next state of the agent and the reward depend on the actions in the previous state",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "cell_id": "1d372b4c2af0420bb69e55da3a88f1ec",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "e14b1da4332944b3be51fb8c37de2b86"
    },
    {
      "cell_type": "markdown",
      "source": "- Which can causing the DQN to be instable",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "bold": true
            },
            "toCodePoint": 40,
            "fromCodePoint": 32
          }
        ],
        "cell_id": "9f371a488be748bfae7d618da8e488ba",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "c2fe4ac023224c1dbc782beaf961a637"
    },
    {
      "cell_type": "markdown",
      "source": "- To get around this we use the experience replay technique which breaks the correlation between subsequent transitions by 1) saving experiences in memory 2) sampling randomly from the stored transition when we make Q-value updates. ",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [
          {
            "type": "marks",
            "marks": {
              "underline": true
            },
            "toCodePoint": 13,
            "fromCodePoint": 3
          }
        ],
        "cell_id": "bee1b8f501e94ae6b4f44596e13cfc83",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "d100c07355e2441ab69b8c5945c63833"
    },
    {
      "cell_type": "markdown",
      "source": "- Important to note is that experience replay has a fixed memory. Therefore if we exceed the replay buffer we then only store the most recent experiences. Override / get rid of older experiences",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "cell_id": "e7fa0620356a45a1843a4e3acff0f8f6",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "331cfb34f7004cfc8b41a43d465e6c1d"
    },
    {
      "cell_type": "markdown",
      "source": "- Look up papers for using dynamic experience replay memory",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "cell_id": "616997bc24ae409389f7a514b8fea884",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "59d876c457cb48e48ffaaada880f6c5e"
    },
    {
      "cell_type": "markdown",
      "source": "- This is one of the tricks Vincent was referring too**",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "cell_id": "a9c11bf39177424a822819c4097c9418",
        "deepnote_cell_type": "text-cell-bullet"
      },
      "block_group": "42bd8649230a487ca52a7417872480d3"
    },
    {
      "cell_type": "code",
      "source": "class ExperienceReplay:\n    def __init__(self, env, buffer_size, min_replay_size = 1000, seed = 123):\n        self.env = env\n        self.min_replay_size = min_replay_size\n        self.replay_buffer = deque(maxlen=buffer_size)\n        # whats this reward buffer for?\n        self.reward_buffer = deque([-200.0], maxlen=100)\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print('Please wait, the experience replay buffer will be filled with random transitions')\n\n        obs, _ = self.env.reset(seed=seed)\n        for _ in range(self.min_replay_size):\n            action = env.action_space.sample()\n            new_obs, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n\n            transition = (obs, action, reward, done, new_obs)\n            self.replay_buffer.append(transition)\n            obs = new_obs\n\n            if done:\n                obs, _ = env.reset(seed=seed)\n\n        print('Initialization with random transitions is done!')\n\n    def add_data(self, data):\n        self.replay_buffer.append(data)\n\n    def sample(self, batch_size):\n        transitions = random.sample(self.replay_buffer, batch_size)\n\n        observations = np.asarray([t[0] for t in transitions])\n        actions = np.asarray([t[1] for t in transitions])\n        rewards = np.asarray([t[2] for t in transitions])\n        dones = np.asarray([t[3] for t in transitions])\n        new_observations = np.asarray([t[4] for t in transitions])\n\n        observations_t = torch.as_tensor(observations, dtype = torch.float32, device=self.device)\n        actions_t = torch.as_tensor(actions, dtype = torch.int64, device=self.device).unsqueeze(-1)\n        rewards_t = torch.as_tensor(rewards, dtype = torch.float32, device=self.device).unsqueeze(-1)\n        dones_t = torch.as_tensor(dones, dtype = torch.float32, device=self.device).unsqueeze(-1)\n        new_observations_t = torch.as_tensor(new_observations, dtype = torch.float32, device=self.device)\n\n        return observations_t, actions_t, rewards_t, dones_t, new_observations_t \n\n    def add_reward(self, reward):\n        self.reward_buffer.append(reward)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698259276840,
        "execution_millis": 35,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "d3eec7e751884e0bb06dda4a045897a6",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "d4938fcd27814f13a88942586f10060c"
    },
    {
      "cell_type": "markdown",
      "source": "### Vanilla DQN Agent",
      "metadata": {
        "is_collapsed": false,
        "formattedRanges": [],
        "cell_id": "2b4d9d74159a42e9991c944fee593b47",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "9b506b476929418881f3b16aa63a7fcc"
    },
    {
      "cell_type": "code",
      "source": "class vanilla_DQNAgent:\n    def __init__(self, env_name, device, epsilon_decay, \n                 epsilon_start, epsilon_end, discount_rate, lr, buffer_size, seed = 123):\n        self.env_name = env_name\n        self.env = gym.make(self.env_name, render_mode = None)\n        self.device = device\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_start = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.discount_rate = discount_rate\n        self.learning_rate = lr\n        self.buffer_size = buffer_size\n        \n        self.replay_memory = ExperienceReplay(self.env, self.buffer_size, seed = seed)\n        self.online_network = DQN(self.env, self.learning_rate).to(self.device)\n\n    def choose_action(self, step, observation, greedy=False):\n        # what does np.interp do?\n        epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n\n        random_sample = random.random()\n\n        if (random_sample <= epsilon) and not greedy:\n            # random action\n            action = self.env.action_space.sample()\n        \n        else:\n            # greedy action\n            obs_t = torch.as_tensor(observation, dtype=torch.float32, device=self.device)\n            # what does unsqueeze do?\n            q_values = self.online_network(obs_t.unsqueeze(0))\n            \n            # arg max ? explain with example\n            max_q_index = torch.argmax(q_values, dim = 1)[0]\n            action = max_q_index.detach().item()\n\n        return action, epsilon\n\n    def learn(self, batch_size):\n        # sample rand transition with size = batch size from reply buffer\n        observations_t, actions_t, rewards_t, dones_t, new_observations_t = self.replay_memory.sample(batch_size)\n\n        target_q_values = self.online_network(new_observations_t)\n        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n\n        targets = rewards_t + self.discount_rate * (1-dones_t) * max_target_q_values\n\n        q_values = self.online_network(observations_t)\n        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n\n        # try different types of loss\n        loss = F.smooth_l1_loss(action_q_values, targets.detach())\n        #loss = F.mse_loss(action_q_values, targets.detach())\n\n        self.online_network.optimizer.zero_grad()\n        loss.backward()\n        self.online_network.optimizer.step()",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698260149178,
        "execution_millis": 20,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "cc0deb068a2b4f0d8ab56c38a87324c5",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "fc921d4c945c4d21874a21b31d1dfc79"
    },
    {
      "cell_type": "markdown",
      "source": "### HyperParameters",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "f21d95d29cd540aa9b789e5376354b7d",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "86a819366c3b40fe9308bf99bc1faca6"
    },
    {
      "cell_type": "code",
      "source": "#Discount rate\ndiscount_rate = 0.99\n#That is the sample that we consider to update our algorithm\nbatch_size = 32\n#Maximum number of transitions that we store in the buffer\nbuffer_size = 50000\n#Minimum number of random transitions stored in the replay buffer\nmin_replay_size = 1000\n#Starting value of epsilon\nepsilon_start = 1.0\n#End value (lowest value) of epsilon\nepsilon_end = 0.05\n#Decay period until epsilon start -> epsilon end\nepsilon_decay = 1000\n\nmax_episodes = 250000\n\n#Learning_rate\nlr = 5e-4",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698260360507,
        "execution_millis": 12,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "8deb0cf8cdec409aaf9d96ccc4bdf938",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "b09fb1c520774024b61e01c764aea070"
    },
    {
      "cell_type": "markdown",
      "source": "### Initialize the agent",
      "metadata": {
        "formattedRanges": [],
        "cell_id": "5e979ba63b6843dbbf6f10002a02ed33",
        "deepnote_cell_type": "text-cell-h3"
      },
      "block_group": "7c71734938d644a3b28a658c4f1446d7"
    },
    {
      "cell_type": "code",
      "source": "env_name = 'MountainCar-v0'\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nvanilla_agent = vanilla_DQNAgent(env_name, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698260363271,
        "execution_millis": 44,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "dae7095cd3d249fd9ef2b9cbaf5abac9",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Please wait, the experience replay buffer will be filled with random transitions\nInitialization with random transitions is done!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "2c6cbdc14b5148788856d1c946ca85ae"
    },
    {
      "cell_type": "code",
      "source": "def training_loop(env_name, agent, max_episodes, target_=False, seed=42):\n    env = gym.make(env_name, render_mode=None)\n    env.action_space.seed(seed)\n    obs, _ = env.reset(seed=seed)\n    # why negative?\n    average_reward_list = [-200]\n    episode_reward = 0.0\n\n    for step in range(max_episodes):\n\n        # choose move greedy or random\n        # what is step used for?\n        action, epsilon = agent.choose_action(step, obs)\n\n        new_obs, reward, terminated, truncated, _ = env.step(action)\n\n        done = terminated or truncated\n\n        transition = (obs, action, reward, done, new_obs)\n\n        agent.replay_memory.add_data(transition)\n\n        obs = new_obs\n\n        episode_reward += reward\n\n        # game over - reinitialize reward\n        if done:\n            obs, _ = env.reset(seed=seed)\n            agent.replay_memory.add_reward(episode_reward)\n            episode_reward = 0.0\n\n        # learning stage\n        agent.learn(batch_size)\n\n        # avg after 100 episodes\n        if (step + 1) % 100 == 0:\n            average_reward_list.append(np.mean(agent.replay_memory.reward_buffer))\n\n        if target_:\n            target_update_frequence = 250\n            if step % target_update_frequence == 0:\n                dagent.update_target_network()\n\n        if (step+1) % 10000 == 0:\n            print(20*'--')\n            print('Step', step)\n            print('Epsilon', epsilon)\n            print('Avg Rew', np.mean(agent.replay_memory.reward_buffer))\n            print()\n\n    return average_reward_list",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698260366102,
        "execution_millis": 13,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "d1f548ca36714c3a93031967e057f873",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "5c6b8584358649a8964a879e5d305605"
    },
    {
      "cell_type": "code",
      "source": "average_rewards_vanilla_dqn = training_loop(env_name, vanilla_agent, max_episodes)",
      "metadata": {
        "source_hash": null,
        "execution_start": 1698260368451,
        "execution_millis": 1245022,
        "deepnote_to_be_reexecuted": false,
        "cell_id": "6156a2368d504d37bd50c1ded6ae1928",
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "----------------------------------------\nStep 9999\nEpsilon 0.05\nAvg Rew -191.83018867924528\n\n----------------------------------------\nStep 19999\nEpsilon 0.05\nAvg Rew -195.67\n\n----------------------------------------\nStep 29999\nEpsilon 0.05\nAvg Rew -199.38\n\n----------------------------------------\nStep 39999\nEpsilon 0.05\nAvg Rew -192.79\n\n----------------------------------------\nStep 49999\nEpsilon 0.05\nAvg Rew -190.63\n\n----------------------------------------\nStep 59999\nEpsilon 0.05\nAvg Rew -193.7\n\n----------------------------------------\nStep 69999\nEpsilon 0.05\nAvg Rew -190.54\n\n----------------------------------------\nStep 79999\nEpsilon 0.05\nAvg Rew -189.91\n\n----------------------------------------\nStep 89999\nEpsilon 0.05\nAvg Rew -192.99\n\n----------------------------------------\nStep 99999\nEpsilon 0.05\nAvg Rew -191.52\n\n----------------------------------------\nStep 109999\nEpsilon 0.05\nAvg Rew -192.03\n\n----------------------------------------\nStep 119999\nEpsilon 0.05\nAvg Rew -194.22\n\n----------------------------------------\nStep 129999\nEpsilon 0.05\nAvg Rew -192.5\n\n----------------------------------------\nStep 139999\nEpsilon 0.05\nAvg Rew -189.82\n\n----------------------------------------\nStep 149999\nEpsilon 0.05\nAvg Rew -187.39\n\n----------------------------------------\nStep 159999\nEpsilon 0.05\nAvg Rew -185.37\n\n----------------------------------------\nStep 169999\nEpsilon 0.05\nAvg Rew -188.14\n\n----------------------------------------\nStep 179999\nEpsilon 0.05\nAvg Rew -191.77\n\n----------------------------------------\nStep 189999\nEpsilon 0.05\nAvg Rew -189.13\n\n----------------------------------------\nStep 199999\nEpsilon 0.05\nAvg Rew -191.2\n\n----------------------------------------\nStep 209999\nEpsilon 0.05\nAvg Rew -195.11\n\n----------------------------------------\nStep 219999\nEpsilon 0.05\nAvg Rew -190.41\n\n----------------------------------------\nStep 229999\nEpsilon 0.05\nAvg Rew -183.79\n\n----------------------------------------\nStep 239999\nEpsilon 0.05\nAvg Rew -174.94\n\n----------------------------------------\nStep 249999\nEpsilon 0.05\nAvg Rew -179.1\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null,
      "block_group": "62e0356e53b5451f9ec543f097e45e9c"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=3a1fff61-e37b-4e25-8498-6d22064c7401' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_notebook_id": "d8d5ba52c95c487ebdc58698bd5e798c",
    "deepnote_persisted_session": {
      "createdAt": "2023-10-25T19:46:25.689Z"
    },
    "deepnote_execution_queue": []
  }
}