{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cell_id": "502bb001118c464693a3460fba401d0a",
        "deepnote_app_is_output_hidden": true,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1650,
        "execution_start": 1698259272008,
        "is_output_hidden": true,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gym==0.26.2\n",
            "  Using cached gym-0.26.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.18.0 in /Users/chrispickett/miniconda3/envs/q-table/lib/python3.11/site-packages (from gym==0.26.2) (1.25.0)\n",
            "Collecting cloudpickle>=1.2.0 (from gym==0.26.2)\n",
            "  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting gym-notices>=0.0.4 (from gym==0.26.2)\n",
            "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
            "Installing collected packages: gym-notices, cloudpickle, gym\n",
            "Successfully installed cloudpickle-3.0.0 gym-0.26.2 gym-notices-0.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip3 install gym==0.26.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cell_id": "056714d2b99f465c9dba6d11c5235f5e",
        "deepnote_app_is_output_hidden": true,
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 3146,
        "execution_start": 1698259273665,
        "is_output_hidden": true,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import deque\n",
        "# from mpl_toolkits import mplot3d\n",
        "# from matplotlib import cm\n",
        "# import pandas as pd\n",
        "# import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "12dade3abfc0488084ef3bde421a76db",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 31,
        "execution_start": 1698259276818,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "# Set the seed for reproducibility\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "aaaf5b6e26a64ab5b003d56b1c170c06",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": []
      },
      "source": [
        "### Simple Deep RL Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "b2e7d6ae6567491da32ed6a0cf97c2c6",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": []
      },
      "source": [
        "- Episodic - fix number of time steps to act in environment, not continuous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "eaa8ac432a9b434c8e3254590aa884d3",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": []
      },
      "source": [
        "- Online - Agent as access to the environment and acts (interacts) on the env in real time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5a181dce18ad476ea05d82fbe0580862",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": []
      },
      "source": [
        "- Mode-free - We don't try to create an internal model of the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cell_id": "dc39fa7fa9134423b7d68c1f6a422f0e",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 28,
        "execution_start": 1698259276830,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, env, learning_rate):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # Input to DNN - input features (observations)\n",
        "        input_features = env.observation_space.shape[0]\n",
        "\n",
        "        # Output to DNN - output actions\n",
        "        action_space = env.action_space.n\n",
        "\n",
        "        # i - 128 - 64 - 32 - o\n",
        "        self.dense1 = nn.Linear(in_features=input_features, out_features=128)\n",
        "        self.dense2 = nn.Linear(in_features=128, out_features=64)\n",
        "        self.dense3 = nn.Linear(in_features=64, out_features=32)\n",
        "        self.dense4 = nn.Linear(in_features=32, out_features=action_space)\n",
        "\n",
        "        # How does the Adam optimizer work?\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Why tanh? try with relu instead of tanh too\n",
        "        output = torch.relu(self.dense1(x))\n",
        "        output = torch.tanh(self.dense2(output))\n",
        "        output = torch.tanh(self.dense3(output))\n",
        "        output = self.dense4(output)\n",
        "        \n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "d910552cab524119af9ac3bc5d0ae97c",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "is_collapsed": false
      },
      "source": [
        "### Experience Replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9698126bd9934b138df7cc84913ef080",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": [],
        "is_collapsed": false
      },
      "source": [
        "- Data assumption for gradient method for training DNN : iid (independent identically distributed) - Not true for reinforcement learning (why?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "1d372b4c2af0420bb69e55da3a88f1ec",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": [],
        "is_collapsed": false
      },
      "source": [
        "- Because the data is highly correlated the next state of the agent and the reward depend on the actions in the previous state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "9f371a488be748bfae7d618da8e488ba",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": [
          {
            "fromCodePoint": 32,
            "marks": {
              "bold": true
            },
            "toCodePoint": 40,
            "type": "marks"
          }
        ],
        "is_collapsed": false
      },
      "source": [
        "- Which can causing the DQN to be instable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "bee1b8f501e94ae6b4f44596e13cfc83",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": [
          {
            "fromCodePoint": 3,
            "marks": {
              "underline": true
            },
            "toCodePoint": 13,
            "type": "marks"
          }
        ],
        "is_collapsed": false
      },
      "source": [
        "- To get around this we use the experience replay technique which breaks the correlation between subsequent transitions by 1) saving experiences in memory 2) sampling randomly from the stored transition when we make Q-value updates. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "e7fa0620356a45a1843a4e3acff0f8f6",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": [],
        "is_collapsed": false
      },
      "source": [
        "- Important to note is that experience replay has a fixed memory. Therefore if we exceed the replay buffer we then only store the most recent experiences. Override / get rid of older experiences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "616997bc24ae409389f7a514b8fea884",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": [],
        "is_collapsed": false
      },
      "source": [
        "- Look up papers for using dynamic experience replay memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a9c11bf39177424a822819c4097c9418",
        "deepnote_cell_type": "text-cell-bullet",
        "formattedRanges": [],
        "is_collapsed": false
      },
      "source": [
        "- This is one of the tricks Vincent was referring too**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cell_id": "d3eec7e751884e0bb06dda4a045897a6",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 35,
        "execution_start": 1698259276840,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "class ExperienceReplay:\n",
        "    def __init__(self, env, buffer_size, min_replay_size = 1000, seed = 123):\n",
        "        self.env = env\n",
        "        self.min_replay_size = min_replay_size\n",
        "        self.replay_buffer = deque(maxlen=buffer_size)\n",
        "        # whats this reward buffer for?\n",
        "        self.reward_buffer = deque([-200.0], maxlen=100)\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print('Please wait, the experience replay buffer will be filled with random transitions')\n",
        "\n",
        "        obs, _ = self.env.reset(seed=seed)\n",
        "        for _ in range(self.min_replay_size):\n",
        "            action = env.action_space.sample()\n",
        "            new_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            transition = (obs, action, reward, done, new_obs)\n",
        "            self.replay_buffer.append(transition)\n",
        "            obs = new_obs\n",
        "\n",
        "            if done:\n",
        "                obs, _ = env.reset(seed=seed)\n",
        "\n",
        "        print('Initialization with random transitions is done!')\n",
        "\n",
        "    def add_data(self, data):\n",
        "        self.replay_buffer.append(data)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        transitions = random.sample(self.replay_buffer, batch_size)\n",
        "\n",
        "        observations = np.asarray([t[0] for t in transitions])\n",
        "        actions = np.asarray([t[1] for t in transitions])\n",
        "        rewards = np.asarray([t[2] for t in transitions])\n",
        "        dones = np.asarray([t[3] for t in transitions])\n",
        "        new_observations = np.asarray([t[4] for t in transitions])\n",
        "\n",
        "        observations_t = torch.as_tensor(observations, dtype = torch.float32, device=self.device)\n",
        "        actions_t = torch.as_tensor(actions, dtype = torch.int64, device=self.device).unsqueeze(-1)\n",
        "        rewards_t = torch.as_tensor(rewards, dtype = torch.float32, device=self.device).unsqueeze(-1)\n",
        "        dones_t = torch.as_tensor(dones, dtype = torch.float32, device=self.device).unsqueeze(-1)\n",
        "        new_observations_t = torch.as_tensor(new_observations, dtype = torch.float32, device=self.device)\n",
        "\n",
        "        return observations_t, actions_t, rewards_t, dones_t, new_observations_t \n",
        "\n",
        "    def add_reward(self, reward):\n",
        "        self.reward_buffer.append(reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "2b4d9d74159a42e9991c944fee593b47",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "is_collapsed": false
      },
      "source": [
        "### Vanilla DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cell_id": "cc0deb068a2b4f0d8ab56c38a87324c5",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 20,
        "execution_start": 1698260149178,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "class vanilla_DQNAgent:\n",
        "    def __init__(self, env_name, device, epsilon_decay, \n",
        "                 epsilon_start, epsilon_end, discount_rate, lr, buffer_size, seed = 123):\n",
        "        self.env_name = env_name\n",
        "        self.env = gym.make(self.env_name, render_mode = None)\n",
        "        self.device = device\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_start = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.discount_rate = discount_rate\n",
        "        self.learning_rate = lr\n",
        "        self.buffer_size = buffer_size\n",
        "        \n",
        "        self.replay_memory = ExperienceReplay(self.env, self.buffer_size, seed = seed)\n",
        "        self.online_network = DQN(self.env, self.learning_rate).to(self.device)\n",
        "\n",
        "    def choose_action(self, step, observation, greedy=False):\n",
        "        # what does np.interp do?\n",
        "        epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
        "\n",
        "        random_sample = random.random()\n",
        "\n",
        "        if (random_sample <= epsilon) and not greedy:\n",
        "            # random action\n",
        "            action = self.env.action_space.sample()\n",
        "        \n",
        "        else:\n",
        "            # greedy action\n",
        "            obs_t = torch.as_tensor(observation, dtype=torch.float32, device=self.device)\n",
        "            # what does unsqueeze do?\n",
        "            q_values = self.online_network(obs_t.unsqueeze(0))\n",
        "            \n",
        "            # arg max ? explain with example\n",
        "            max_q_index = torch.argmax(q_values, dim = 1)[0]\n",
        "            action = max_q_index.detach().item()\n",
        "\n",
        "        return action, epsilon\n",
        "\n",
        "    def learn(self, batch_size):\n",
        "        # sample rand transition with size = batch size from reply buffer\n",
        "        observations_t, actions_t, rewards_t, dones_t, new_observations_t = self.replay_memory.sample(batch_size)\n",
        "\n",
        "        target_q_values = self.online_network(new_observations_t)\n",
        "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
        "\n",
        "        targets = rewards_t + self.discount_rate * (1-dones_t) * max_target_q_values\n",
        "\n",
        "        q_values = self.online_network(observations_t)\n",
        "        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
        "\n",
        "        # try different types of loss\n",
        "        loss = F.smooth_l1_loss(action_q_values, targets.detach())\n",
        "        #loss = F.mse_loss(action_q_values, targets.detach())\n",
        "\n",
        "        self.online_network.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.online_network.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "f21d95d29cd540aa9b789e5376354b7d",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": []
      },
      "source": [
        "### HyperParameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cell_id": "8deb0cf8cdec409aaf9d96ccc4bdf938",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 12,
        "execution_start": 1698260360507,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "#Discount rate\n",
        "discount_rate = 0.99\n",
        "#That is the sample that we consider to update our algorithm\n",
        "batch_size = 32\n",
        "#Maximum number of transitions that we store in the buffer\n",
        "buffer_size = 50000\n",
        "#Minimum number of random transitions stored in the replay buffer\n",
        "min_replay_size = 1000\n",
        "#Starting value of epsilon\n",
        "epsilon_start = 1.0\n",
        "#End value (lowest value) of epsilon\n",
        "epsilon_end = 0.05\n",
        "#Decay period until epsilon start -> epsilon end\n",
        "epsilon_decay = 1000\n",
        "\n",
        "max_episodes = 250000\n",
        "\n",
        "#Learning_rate\n",
        "lr = 5e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5e979ba63b6843dbbf6f10002a02ed33",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": []
      },
      "source": [
        "### Initialize the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cell_id": "dae7095cd3d249fd9ef2b9cbaf5abac9",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 44,
        "execution_start": 1698260363271,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please wait, the experience replay buffer will be filled with random transitions\n",
            "Initialization with random transitions is done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/chrispickett/miniconda3/envs/q-table/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "env_name = 'MountainCar-v0'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vanilla_agent = vanilla_DQNAgent(env_name, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cell_id": "d1f548ca36714c3a93031967e057f873",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 13,
        "execution_start": 1698260366102,
        "source_hash": null
      },
      "outputs": [],
      "source": [
        "def training_loop(env_name, agent, max_episodes, target_=False, seed=42):\n",
        "    env = gym.make(env_name, render_mode=None)\n",
        "    env.action_space.seed(seed)\n",
        "    obs, _ = env.reset(seed=seed)\n",
        "    # why negative?\n",
        "    average_reward_list = [-200]\n",
        "    episode_reward = 0.0\n",
        "\n",
        "    for step in range(max_episodes):\n",
        "\n",
        "        # choose move greedy or random\n",
        "        # what is step used for?\n",
        "        action, epsilon = agent.choose_action(step, obs)\n",
        "\n",
        "        new_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        done = terminated or truncated\n",
        "\n",
        "        transition = (obs, action, reward, done, new_obs)\n",
        "\n",
        "        agent.replay_memory.add_data(transition)\n",
        "\n",
        "        obs = new_obs\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # game over - reinitialize reward\n",
        "        if done:\n",
        "            obs, _ = env.reset(seed=seed)\n",
        "            agent.replay_memory.add_reward(episode_reward)\n",
        "            episode_reward = 0.0\n",
        "\n",
        "        # learning stage\n",
        "        agent.learn(batch_size)\n",
        "\n",
        "        # avg after 100 episodes\n",
        "        if (step + 1) % 100 == 0:\n",
        "            average_reward_list.append(np.mean(agent.replay_memory.reward_buffer))\n",
        "\n",
        "        if target_:\n",
        "            target_update_frequence = 250\n",
        "            if step % target_update_frequence == 0:\n",
        "                dagent.update_target_network()\n",
        "\n",
        "        if (step+1) % 10000 == 0:\n",
        "            print(20*'--')\n",
        "            print('Step', step)\n",
        "            print('Epsilon', epsilon)\n",
        "            print('Avg Rew', np.mean(agent.replay_memory.reward_buffer))\n",
        "            print()\n",
        "\n",
        "    return average_reward_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "6156a2368d504d37bd50c1ded6ae1928",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1245022,
        "execution_start": 1698260368451,
        "source_hash": null
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Step 9999\n",
            "Epsilon 0.05\n",
            "Avg Rew -198.66666666666666\n",
            "\n",
            "----------------------------------------\n",
            "Step 19999\n",
            "Epsilon 0.05\n",
            "Avg Rew -199.32\n",
            "\n",
            "----------------------------------------\n",
            "Step 29999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 39999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 49999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 59999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 69999\n",
            "Epsilon 0.05\n",
            "Avg Rew -199.79\n",
            "\n",
            "----------------------------------------\n",
            "Step 79999\n",
            "Epsilon 0.05\n",
            "Avg Rew -199.79\n",
            "\n",
            "----------------------------------------\n",
            "Step 89999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 99999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 109999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 119999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 129999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 139999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 149999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 159999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 169999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 179999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 189999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 199999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 209999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 219999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 229999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 239999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n",
            "----------------------------------------\n",
            "Step 249999\n",
            "Epsilon 0.05\n",
            "Avg Rew -200.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "average_rewards_vanilla_dqn = training_loop(env_name, vanilla_agent, max_episodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test\n"
          ]
        }
      ],
      "source": [
        "print('test')"
      ]
    }
  ],
  "metadata": {
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "d8d5ba52c95c487ebdc58698bd5e798c",
    "deepnote_persisted_session": {
      "createdAt": "2023-10-25T19:46:25.689Z"
    },
    "kernelspec": {
      "display_name": "q-table",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
